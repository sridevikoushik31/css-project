{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project: Echoes of Power\n",
    "=====\n",
    "\n",
    "In this problem set, you will use iPython notebook and various python tools to read data and analyze it.\n",
    "\n",
    "We will use data from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utterances = {}\n",
    "def load_data():\n",
    "    with open('wikipedia.talkpages.conversations.txt', \"r\") as f:\n",
    "        for line in f:\n",
    "            split = line.split(\"+++$+++\")\n",
    "#             print len(split)\n",
    "            try:\n",
    "                utterances[split[0].strip()] = {\"user_id\": split[1].strip(),\n",
    "                                        \"talkpage_user\": split[2].strip(), \n",
    "                                        \"conversation_root\": split[3].strip(),\n",
    "                                        \"reply_to\": split[4].strip(),\n",
    "                                        \"timestamp\": split[5].strip(),\n",
    "                                        \"timestamp_unix\": split[6].strip(),\n",
    "#                                       looks like readme has a small mistake                                     \n",
    "                                        \"raw_text\": split[7].strip(),\n",
    "                                        \"text\": split[8].strip(),        \n",
    "                                        }\n",
    "            except IndexError:\n",
    "                pass\n",
    "load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utterances.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125292\n"
     ]
    }
   ],
   "source": [
    "conversations = {}\n",
    "for u_id, utter in utterances.iteritems():\n",
    "    user_id = utter[\"user_id\"]\n",
    "    reply_id = utter.get(\"reply_to\")\n",
    "#     print utterances.get(reply_id)\n",
    "#     print reply_id\n",
    "    root_conversation = utter[\"conversation_root\"]\n",
    "   \n",
    "#   doubt\n",
    "    if root_conversation == u_id:\n",
    "        continue\n",
    "    if conversations.get(root_conversation) == None:\n",
    "        conversations[root_conversation] = [u_id]\n",
    "    else:\n",
    "        conversations[root_conversation].append(u_id)\n",
    "        \n",
    "        \n",
    "#     if reply_id and utterances.get(reply_id):\n",
    "#         reply_user_id = utterances[reply_id][\"user_id\"]\n",
    "#         if(conversations.get((user_id, reply_user_id)) == None):\n",
    "#             conversations[utter[\"conversation_root\"]] = [reply_id, u_id]\n",
    "#         else:\n",
    "#             conversations[(user_id, reply_user_id)].append(reply_id)\n",
    "#             conversations[(user_id, reply_user_id)].append(u_id)\n",
    "conversations\n",
    "print len(conversations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0426763081442\n"
     ]
    }
   ],
   "source": [
    "count = 0.0\n",
    "for i in conversations.keys():\n",
    "    if utterances[i][\"user_id\"] == utterances[i][\"talkpage_user\"]:\n",
    "        count += 1\n",
    "print count/125292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in utterances.keys():\n",
    "    if(utterances[i][\"reply_to\"] == i):\n",
    "        cnt += 1\n",
    "#     print utterances[i][\"reply_to\"]\n",
    "#     pass\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_root': '577840', 'user_id': 'Zanimum', 'talkpage_user': 'Zanimum', 'timestamp_unix': '1.179627540E09', 'reply_to': '577842', 'timestamp': '2007-05-19 19:19:00', 'text': \"::: No, I'll keep it. :) Just wanted to make sure you knew I didn't click the camera myself. Anyway, thanks for noticing my efforts.  --  [[User:Zanimum|Zanimum]] 19:19, 19 May 2007 (UTC)\", 'raw_text': \"No, I'll keep it. :) Just wanted to make sure you knew I didn't click the camera myself. Anyway, thanks for noticing my efforts.  --\"}\n",
      "577841\n",
      "{'conversation_root': '577840', 'user_id': 'Zanimum', 'talkpage_user': 'Zanimum', 'timestamp_unix': '1.179626880E09', 'reply_to': '577840', 'timestamp': '2007-05-19 19:08:00', 'text': ': [Blush] I don\\'t know if I do... they were taken by other volunteer photographers, that I \"commissioned\". I\\'ve been aranging as many photo shoots as possible, to get this type of image.  --  [[User:Zanimum|Zanimum]] 19:08, 19 May 2007 (UTC)', 'raw_text': '[Blush] I don\\'t know if I do... they were taken by other volunteer photographers, that I \\\\\"commissioned\\\\\". I\\'ve been aranging as many photo shoots as possible, to get this type of image.  --'}\n",
      "{'conversation_root': '577840', 'user_id': 'QuasyBoy', 'talkpage_user': 'Zanimum', 'timestamp_unix': '1.179608280E09', 'reply_to': 'initial_post', 'timestamp': '2007-05-19 13:58:00', 'text': '|style=\"vertical-align: middle; border-top: 1px solid gray;\" | I, QuasyBoy, award Zanimum this award for his awesome free images of [[Hilary Duff]] and [[Christy Carlson Romano]]. Keep up the good work, Bro. [[User:QuasyBoy|QuasyBoy]] 13:58, 19 May 2007 (UTC)', 'raw_text': '|style=\\\\\"vertical-align: middle; border-top: 1px solid gray;\\\\\" | I, QuasyBoy, award Zanimum this award for his awesome free images of [[Hilary Duff]] and [[Christy Carlson Romano]]. Keep up the good work, Bro.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#'577840': ['577843', '577842', '577841', '577844']]\n",
    "print utterances['577843']\n",
    "print (utterances['577842']['reply_to'])\n",
    "print utterances['577841']\n",
    "# print utterances['577844']['reply_to']\n",
    "print utterances['577840']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_other_user(conversation_list, root_user):\n",
    "    for i in conversation_list:\n",
    "        other_user = utterances[i][\"user_id\"]\n",
    "        if utterances[i][\"user_id\"] != root_user and other_user != \"\":\n",
    "            return other_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['320591', '320592', '320593', '320594', '320595', '320596', '320597']\n"
     ]
    }
   ],
   "source": [
    "conv_list = ['445339','445338','445337','445336','445335','445344','445345','445342','445343','445340','445341']\n",
    "# '228054': ['228055', '228056']\n",
    "conv_list = ['228054','228055', '228056']\n",
    "conv_list = ['320591', '320597', '320595', '320594', '320593', '320592', '320596']\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, u_id):\n",
    "        self.u_id = u_id\n",
    "        self.child = None\n",
    "        self.parent = None\n",
    "#     def add_child(self, child_node=None, parent_node=None):\n",
    "#         self.child = child_node\n",
    "#         self.parent = parent_node\n",
    "#         return True\n",
    "\n",
    "\n",
    "def get_root(node_list, start_node):\n",
    "    if start_node.parent == None:\n",
    "        return start_node\n",
    "    else:\n",
    "        parent = start_node.parent\n",
    "        return get_root(node_list, node_list[parent])\n",
    "\n",
    "\n",
    "def compute_conversation_hierarchy(conv_list):\n",
    "    node_list = {}\n",
    "    \n",
    "    for i in conv_list:\n",
    "        node_list[i] = Node(i)\n",
    "        \n",
    "    for i in conv_list:\n",
    "        reply_to = utterances[i][\"reply_to\"]\n",
    "        if reply_to != '-1' and reply_to != \"initial_post\":\n",
    "            node_list[i].parent = reply_to\n",
    "            node_list[reply_to].child = i\n",
    "#           doubt : if a message has more than 1 replies then we are ignosing that here\n",
    "#                 : \n",
    "        \n",
    "        \n",
    "    root = get_root(node_list, node_list.values()[0])\n",
    "    new_list = [root.u_id]\n",
    "    cur = root\n",
    "    while(len(new_list) < len(node_list) and cur.child != None):\n",
    "        cur = node_list[cur.child]\n",
    "        new_list.append(cur.u_id)\n",
    "        \n",
    "    return new_list\n",
    "\n",
    "print compute_conversation_hierarchy(conv_list)\n",
    "\n",
    "#     for i in node_list.values():\n",
    "#         print i.u_id\n",
    "#         print \"its child is %s\" % i.child\n",
    "\n",
    "#         print \"its parent is %s\" % i.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "final_conversations = {}\n",
    "\n",
    "for i, j in conversations.iteritems():\n",
    "    conv_list = copy.deepcopy(j)\n",
    "    conv_list.insert(0, i)\n",
    "    conv_hierarchy = compute_conversation_hierarchy(conv_list)\n",
    "    B_user = utterances[i][\"talkpage_user\"]\n",
    "    A_user = utterances[i][\"user_id\"]\n",
    "    if final_conversations.get((A_user, B_user)):\n",
    "        final_conversations[(A_user, B_user)].append(conv_hierarchy)\n",
    "    else:\n",
    "        final_conversations[(A_user, B_user)] = [conv_hierarchy]\n",
    "#     break\n",
    "    \n",
    "# print final_conversations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9ec964bfbc64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mget_ordered_conversation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-9ec964bfbc64>\u001b[0m in \u001b[0;36mget_ordered_conversation\u001b[1;34m(conv_list)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mconv_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mmake_conv_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconv_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-9ec964bfbc64>\u001b[0m in \u001b[0;36mmake_conv_list\u001b[1;34m(conv_list)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_conv_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcur_conv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mreply_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutterances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reply_to\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def append_to_left_of(item, li, item_to_insert, conv_list):\n",
    "    if(item in li):\n",
    "        index= li.index(item)\n",
    "        li.insert(index, item_to_insert)\n",
    "        conv_list.remove\n",
    "\n",
    "def make_conv_list(conv_list):\n",
    "    cur_conv = [conv_list[0]]\n",
    "    for c in range(1,len(conv_list)):\n",
    "        reply_to = utterances[c][\"reply_to\"]\n",
    "        \n",
    "\n",
    "conv_list = ['445339','445338','445337','445336','445335','445344','445345','445342','445343','445340','445341']\n",
    "\n",
    "def get_ordered_conversation(conv_list):\n",
    "    temp = []\n",
    "    while(len(conv_list) > 0):\n",
    "        conv_list = []\n",
    "        make_conv_list(conv_list)\n",
    "    \n",
    "    for c in conv_list:\n",
    "        reply_to = utterances[c][\"reply_to\"]\n",
    "        append_to_left_of\n",
    "        print reply_to\n",
    "        \n",
    "        \n",
    "get_ordered_conversation(conv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_left = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-243723baf121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mroot_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutterances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"user_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mother_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_other_user\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_user\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mconversation_threads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_user\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_user\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ordered_conversation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mconversation_threads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-9ec964bfbc64>\u001b[0m in \u001b[0;36mget_ordered_conversation\u001b[1;34m(conv_list)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mconv_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mmake_conv_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconv_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-9ec964bfbc64>\u001b[0m in \u001b[0;36mmake_conv_list\u001b[1;34m(conv_list)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_conv_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcur_conv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mreply_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutterances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reply_to\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "conversation_threads = {}\n",
    "for i, v in conversations.iteritems():\n",
    "    root_user = utterances[i][\"user_id\"]\n",
    "    other_user = get_other_user(v, root_user)\n",
    "    conversation_threads[(root_user, other_user)] = get_ordered_conversation(v)\n",
    "conversation_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk import sent_tokenize,word_tokenize,porter\n",
    "\n",
    "import csv\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'although', 'and', 'as', 'as far as', 'as how', 'as if', 'as long as', 'as soon as', 'as though', 'as well as', 'because', 'before', 'both', 'but', 'either', 'even if', 'even though', 'for', 'how', 'however', 'if', 'if only', 'in case', 'in order that', 'neither', 'nor', 'now', 'once', 'only', 'or', 'provided', 'rather than', 'since', 'so', 'so that', 'than', 'that', 'though', 'till', 'unless', 'until', 'when', 'whenever', 'where', 'whereas', 'wherever', 'whether', 'while', 'yet']\n"
     ]
    }
   ],
   "source": [
    "def word_proc(x, preProcessing_type):\n",
    "    \n",
    "    if (preProcessing_type == 0): return x\n",
    "    if (preProcessing_type == 1): return x.lower()\n",
    "    if (preProcessing_type == 2): \n",
    "        stemmer = PorterStemmer()\n",
    "        x = x.lower()\n",
    "        return stemmer.stem(x)\n",
    "\n",
    "    return x # default \n",
    "\n",
    "def create_markers(category):\n",
    "\n",
    "    file = open(category+'.txt', \"r\")\n",
    "    markers = file.read()\n",
    "    \n",
    "    tokens = (re.split('[\\t\\n]', markers.strip()))\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "\n",
    "conjunction_list = create_markers('conjunctions')  \n",
    "\n",
    "print conjunction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -10, 'personal_pronouns': -10, 'conjunctions': -10}\n",
      "{'articles': -10, 'personal_pronouns': -10, 'conjunctions': -10}\n",
      "{'articles': 0.5, 'personal_pronouns': 0.5, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -0.5, 'personal_pronouns': -0.5, 'conjunctions': -0.5}\n",
      "{'articles': -10, 'personal_pronouns': -10, 'conjunctions': -10}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -1.0, 'personal_pronouns': 0.0, 'conjunctions': -0.5}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -0.3333333333333333, 'personal_pronouns': -0.16666666666666663, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -10, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -0.15555555555555556, 'personal_pronouns': -0.30000000000000004, 'conjunctions': -0.2196078431372549}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': -10, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -0.35, 'personal_pronouns': -0.19999999999999996, 'conjunctions': -0.15000000000000002}\n",
      "{'articles': -10, 'personal_pronouns': -10, 'conjunctions': -10}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -10, 'personal_pronouns': -10, 'conjunctions': -10}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -0.75, 'personal_pronouns': -0.6666666666666667, 'conjunctions': -0.75}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': -0.16666666666666663}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': -0.16666666666666669, 'personal_pronouns': -0.5, 'conjunctions': -0.33333333333333337}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': -0.5, 'conjunctions': 0.0}\n",
      "{'articles': 0.0, 'personal_pronouns': 0.0, 'conjunctions': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# code to compute coordination between user a and user b\n",
    "\n",
    "\"\"\"('Tpbradbury', 'Karanacs'): [['357026', '357027', '357028', '357029']], \n",
    "    ('Skope', 'Jedi6'):         [['500004', '500005'], ['375918', '375919'], ['500181', '500183'], \n",
    "                                 ['500029', '500030', '500031', '500032']]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "utterances = {}\n",
    "                utterances[split[0].strip()] = {\"user_id\": split[1].strip(),\n",
    "                                        \"talkpage_user\": split[2].strip(), \n",
    "                                        \"conversation_root\": split[3].strip(),\n",
    "                                        \"reply_to\": split[4].strip(),\n",
    "                                        \"timestamp\": split[5].strip(),\n",
    "                                        \"timestamp_unix\": split[6].strip(),\n",
    "#                                       looks like readme has a small mistake                                     \n",
    "                                        \"raw_text\": split[7].strip(),\n",
    "                                        \"text\": split[8].strip(),        \n",
    "                                        \n",
    "\"\"\"\n",
    "\n",
    "marker_list = {}\n",
    "utterances_tokenized = {}\n",
    "\n",
    "def tokenize(utterance):\n",
    "    preProcessing_type = 1\n",
    "    \n",
    "    text = utterances[utterance][\"text\"]\n",
    "    \n",
    "    word_list = (text.strip()).split(' ')\n",
    "    \n",
    "    tokens = Counter()\n",
    "    \n",
    "    \n",
    "    # print 'text',text\n",
    "    \n",
    "    # for word in word_tokenize(text):\n",
    "    for word in word_list:\n",
    "        tokens[word_proc(word, preProcessing_type)] += 1\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def contains(utterance, marker):\n",
    "    \n",
    "    preProcessing_type = 1\n",
    "    \n",
    "    if (utterance not in utterances_tokenized):\n",
    "        utterances_tokenized[utterance] = tokenize(utterance) # returns dictionary with keys as tokens\n",
    "        \n",
    "    # print utterances_tokenized\n",
    "\n",
    "    # print \"--------------------------------------\"\n",
    "    \n",
    "    word_list = marker_list[marker]\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        word = word_proc(word_list[i], preProcessing_type)\n",
    "        if (word in utterances_tokenized[utterance]):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "for consistency with prior work, we employed eight of the nine\n",
    "LIWC-derived categories [41] deemed to be processed by humans\n",
    "in a generally non-conscious fashion [28]. Our eight markers are\n",
    "thus: articles, auxiliary verbs, conjunctions, high-frequency ad-\n",
    "verbs, impersonal pronouns, personal pronouns, prepositions, and\n",
    "quantifiers (451 lexemes total)\n",
    "\"\"\"    \n",
    "\n",
    "# input: list of utterance list. Each utterance list is a conversation   \n",
    "def compute_coordination(conv_list):\n",
    "    \n",
    "    marker_list['articles'] =  ['a', 'an', 'the']\n",
    "    marker_list['personal_pronouns'] = ['i','them','her','you','he','she','it','they','we']\n",
    "    marker_list['conjunctions'] = create_markers('conjunctions')\n",
    "    \n",
    "    \n",
    "    coordination_list = {}\n",
    "    \n",
    "    for marker in marker_list.keys():\n",
    "        \n",
    "        a_usage_count = 0\n",
    "        b_usage_count = 0\n",
    "        \n",
    "        b_follows_a_count = 0\n",
    "        b_utterance_count = 0\n",
    "        \n",
    "        for conversation in conv_list:\n",
    "            for i in range(len(conversation)): #['500029', '500030', '500031', '500032']\n",
    "                if (i%2 != 0):continue # for skipping B's utterance\n",
    "                    \n",
    "                 \n",
    "                a_utterance = conversation[i]\n",
    "                b_utterance = None\n",
    "                if (i+1 < len(conversation)):\n",
    "                        b_utterance = conversation[i+1]\n",
    "                        b_utterance_count +=1\n",
    "            \n",
    "                a_utterance_has_m = contains(a_utterance, marker)\n",
    "                \n",
    "                b_utterance_has_m = False\n",
    "                if (b_utterance != None):\n",
    "                    b_utterance_has_m = contains(b_utterance, marker)\n",
    "                \n",
    "                if (a_utterance_has_m):\n",
    "                    a_usage_count +=1\n",
    "                    \n",
    "                if (b_utterance_has_m):    \n",
    "                    b_usage_count +=1\n",
    "                    \n",
    "                if (a_utterance_has_m and b_utterance_has_m):\n",
    "                    b_follows_a_count +=1\n",
    "       \n",
    "    \n",
    "#        print 'marker',marker\n",
    "#        print 'b_follows_a_count',b_follows_a_count\n",
    "#        print 'a_usage_count',a_usage_count \n",
    "#        print  'b_usage_count',b_usage_count\n",
    "#        print 'b_utterance_count',b_utterance_count \n",
    "#        print '----------------------'\n",
    "        if (a_usage_count == 0 or b_utterance_count == 0): \n",
    "            coordination = -10 # invalid\n",
    "        else:\n",
    "            coordination = 1.0*b_follows_a_count/a_usage_count - 1.0*b_usage_count/b_utterance_count\n",
    "        \n",
    "        coordination_list[marker] = coordination\n",
    "                # if ()\n",
    "    \n",
    "    return coordination_list\n",
    "   \n",
    " \n",
    "# test code : -10 indicates cannot be computed \n",
    "i = 0\n",
    "for key in final_conversations.keys():    \n",
    "    i +=1\n",
    "    if (i > 40): break\n",
    "        \n",
    "    print compute_coordination(final_conversations[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
