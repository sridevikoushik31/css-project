{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project: Echoes of Power\n",
    "=====\n",
    "\n",
    "In this problem set, you will use iPython notebook and various python tools to read data and analyze it.\n",
    "\n",
    "We will use data from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utterances = {}\n",
    "def load_data():\n",
    "    with open('wikipedia.talkpages.conversations.txt', \"r\") as f:\n",
    "        for line in f:\n",
    "            split = line.split(\"+++$+++\")\n",
    "#             print len(split)\n",
    "            try:\n",
    "                utterances[split[0].strip()] = {\"user_id\": split[1].strip(),\n",
    "                                        \"talkpage_user\": split[2].strip(), \n",
    "                                        \"conversation_root\": split[3].strip(),\n",
    "                                        \"reply_to\": split[4].strip(),\n",
    "                                        \"timestamp\": split[5].strip(),\n",
    "                                        \"timestamp_unix\": split[6].strip(),\n",
    "#                                       looks like readme has a small mistake                                     \n",
    "                                        \"raw_text\": split[7].strip(),\n",
    "                                        \"text\": split[8].strip(),        \n",
    "                                        }\n",
    "            except IndexError:\n",
    "                pass\n",
    "load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utterances.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125292\n"
     ]
    }
   ],
   "source": [
    "conversations = {}\n",
    "for u_id, utter in utterances.iteritems():\n",
    "    user_id = utter[\"user_id\"]\n",
    "    reply_id = utter.get(\"reply_to\")\n",
    "#     print utterances.get(reply_id)\n",
    "#     print reply_id\n",
    "    root_conversation = utter[\"conversation_root\"]\n",
    "   \n",
    "#   doubt\n",
    "    if root_conversation == u_id:\n",
    "        continue\n",
    "    if conversations.get(root_conversation) == None:\n",
    "        conversations[root_conversation] = [u_id]\n",
    "    else:\n",
    "        conversations[root_conversation].append(u_id)\n",
    "        \n",
    "        \n",
    "#     if reply_id and utterances.get(reply_id):\n",
    "#         reply_user_id = utterances[reply_id][\"user_id\"]\n",
    "#         if(conversations.get((user_id, reply_user_id)) == None):\n",
    "#             conversations[utter[\"conversation_root\"]] = [reply_id, u_id]\n",
    "#         else:\n",
    "#             conversations[(user_id, reply_user_id)].append(reply_id)\n",
    "#             conversations[(user_id, reply_user_id)].append(u_id)\n",
    "conversations\n",
    "print len(conversations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0426763081442\n"
     ]
    }
   ],
   "source": [
    "count = 0.0\n",
    "for i in conversations.keys():\n",
    "    if utterances[i][\"user_id\"] == utterances[i][\"talkpage_user\"]:\n",
    "        count += 1\n",
    "print count/125292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in utterances.keys():\n",
    "    if(utterances[i][\"reply_to\"] == i):\n",
    "        cnt += 1\n",
    "#     print utterances[i][\"reply_to\"]\n",
    "#     pass\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_root': '577840', 'user_id': 'Zanimum', 'talkpage_user': 'Zanimum', 'timestamp_unix': '1.179627540E09', 'reply_to': '577842', 'timestamp': '2007-05-19 19:19:00', 'text': \"::: No, I'll keep it. :) Just wanted to make sure you knew I didn't click the camera myself. Anyway, thanks for noticing my efforts.  --  [[User:Zanimum|Zanimum]] 19:19, 19 May 2007 (UTC)\", 'raw_text': \"No, I'll keep it. :) Just wanted to make sure you knew I didn't click the camera myself. Anyway, thanks for noticing my efforts.  --\"}\n",
      "577841\n",
      "{'conversation_root': '577840', 'user_id': 'Zanimum', 'talkpage_user': 'Zanimum', 'timestamp_unix': '1.179626880E09', 'reply_to': '577840', 'timestamp': '2007-05-19 19:08:00', 'text': ': [Blush] I don\\'t know if I do... they were taken by other volunteer photographers, that I \"commissioned\". I\\'ve been aranging as many photo shoots as possible, to get this type of image.  --  [[User:Zanimum|Zanimum]] 19:08, 19 May 2007 (UTC)', 'raw_text': '[Blush] I don\\'t know if I do... they were taken by other volunteer photographers, that I \\\\\"commissioned\\\\\". I\\'ve been aranging as many photo shoots as possible, to get this type of image.  --'}\n",
      "{'conversation_root': '577840', 'user_id': 'QuasyBoy', 'talkpage_user': 'Zanimum', 'timestamp_unix': '1.179608280E09', 'reply_to': 'initial_post', 'timestamp': '2007-05-19 13:58:00', 'text': '|style=\"vertical-align: middle; border-top: 1px solid gray;\" | I, QuasyBoy, award Zanimum this award for his awesome free images of [[Hilary Duff]] and [[Christy Carlson Romano]]. Keep up the good work, Bro. [[User:QuasyBoy|QuasyBoy]] 13:58, 19 May 2007 (UTC)', 'raw_text': '|style=\\\\\"vertical-align: middle; border-top: 1px solid gray;\\\\\" | I, QuasyBoy, award Zanimum this award for his awesome free images of [[Hilary Duff]] and [[Christy Carlson Romano]]. Keep up the good work, Bro.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#'577840': ['577843', '577842', '577841', '577844']]\n",
    "print utterances['577843']\n",
    "print (utterances['577842']['reply_to'])\n",
    "print utterances['577841']\n",
    "# print utterances['577844']['reply_to']\n",
    "print utterances['577840']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_other_user(conversation_list, root_user):\n",
    "    for i in conversation_list:\n",
    "        other_user = utterances[i][\"user_id\"]\n",
    "        if utterances[i][\"user_id\"] != root_user and other_user != \"\":\n",
    "            return other_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['320591', '320592', '320593', '320594', '320595', '320596', '320597']\n"
     ]
    }
   ],
   "source": [
    "conv_list = ['445339','445338','445337','445336','445335','445344','445345','445342','445343','445340','445341']\n",
    "# '228054': ['228055', '228056']\n",
    "conv_list = ['228054','228055', '228056']\n",
    "conv_list = ['320591', '320597', '320595', '320594', '320593', '320592', '320596']\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, u_id):\n",
    "        self.u_id = u_id\n",
    "        self.child = None\n",
    "        self.parent = None\n",
    "#     def add_child(self, child_node=None, parent_node=None):\n",
    "#         self.child = child_node\n",
    "#         self.parent = parent_node\n",
    "#         return True\n",
    "\n",
    "\n",
    "def get_root(node_list, start_node):\n",
    "    if start_node.parent == None:\n",
    "        return start_node\n",
    "    else:\n",
    "        parent = start_node.parent\n",
    "        return get_root(node_list, node_list[parent])\n",
    "\n",
    "\n",
    "def compute_conversation_hierarchy(conv_list):\n",
    "    node_list = {}\n",
    "    \n",
    "    for i in conv_list:\n",
    "        node_list[i] = Node(i)\n",
    "        \n",
    "    for i in conv_list:\n",
    "        reply_to = utterances[i][\"reply_to\"]\n",
    "        if reply_to != '-1' and reply_to != \"initial_post\":\n",
    "            node_list[i].parent = reply_to\n",
    "            node_list[reply_to].child = i\n",
    "#           doubt : if a message has more than 1 replies then we are ignosing that here\n",
    "#                 : \n",
    "        \n",
    "        \n",
    "    root = get_root(node_list, node_list.values()[0])\n",
    "    new_list = [root.u_id]\n",
    "    cur = root\n",
    "    while(len(new_list) < len(node_list) and cur.child != None):\n",
    "        cur = node_list[cur.child]\n",
    "        new_list.append(cur.u_id)\n",
    "        \n",
    "    return new_list\n",
    "\n",
    "print compute_conversation_hierarchy(conv_list)\n",
    "\n",
    "#     for i in node_list.values():\n",
    "#         print i.u_id\n",
    "#         print \"its child is %s\" % i.child\n",
    "\n",
    "#         print \"its parent is %s\" % i.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "final_conversations = {}\n",
    "\n",
    "user_pairs = {}\n",
    "\n",
    "# break_counter = 0\n",
    "\n",
    "for i, j in conversations.iteritems():\n",
    "     \n",
    "    # break_counter +=1\n",
    "    \n",
    "    # if (break_counter > 5): break;\n",
    "        \n",
    "    # print 'i,j=',i,j    \n",
    "        \n",
    "    conv_list = copy.deepcopy(j)\n",
    "    conv_list.insert(0, i)\n",
    "    conv_hierarchy = compute_conversation_hierarchy(conv_list)\n",
    "    \n",
    "    u_index = conv_hierarchy[0]\n",
    "    \n",
    "    B_user = utterances[u_index][\"talkpage_user\"]\n",
    "    A_user = utterances[u_index][\"user_id\"]\n",
    "    \n",
    "    \n",
    "    # assumption validation code\n",
    "    inValid_conv = False\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    for u_id in conv_hierarchy:\n",
    "        if ((counter%2 == 0) and (utterances[u_id]['user_id'] != A_user)): \n",
    "            inValid_conv = True\n",
    "            break;\n",
    "            \n",
    "        if (((counter%2 != 0)) and (utterances[u_id]['user_id'] != B_user)): \n",
    "            inValid_conv = True\n",
    "            break;\n",
    "            \n",
    "        counter +=1\n",
    "        \n",
    "        \n",
    "    if (inValid_conv):\n",
    "        continue\n",
    "    \n",
    "    if ((A_user, B_user)) not in user_pairs:\n",
    "        user_pairs[(A_user, B_user)] = 1\n",
    "    \n",
    "    if final_conversations.get((A_user, B_user)):\n",
    "        final_conversations[(A_user, B_user)].append(conv_hierarchy)\n",
    "    else:\n",
    "        final_conversations[(A_user, B_user)] = [conv_hierarchy]\n",
    "#     break\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conversations =  47602\n",
      "('Sluzzelin', 'Wavelength')\n",
      "------------------------------\n",
      "('Sluzzelin', 'Wavelength')\n",
      "32501\n",
      "Sluzzelin\n",
      "****\n",
      "32502\n",
      "Wavelength\n",
      "****\n",
      "32503\n",
      "Sluzzelin\n",
      "****\n",
      "------------------------------------------\n",
      "('Unschool', 'Muchness')\n",
      "283621\n",
      "Unschool\n",
      "****\n",
      "283622\n",
      "Muchness\n",
      "****\n",
      "------------------------------------------\n",
      "('RHaworth', 'Gronky')\n",
      "36730\n",
      "RHaworth\n",
      "****\n",
      "36731\n",
      "Gronky\n",
      "****\n",
      "------------------------------------------\n",
      "('MichaelQSchmidt', 'Hullaballoo Wolfowitz')\n",
      "61719\n",
      "MichaelQSchmidt\n",
      "****\n",
      "61720\n",
      "Hullaballoo Wolfowitz\n",
      "****\n",
      "------------------------------------------\n",
      "('MER-C', 'Jjron')\n",
      "331690\n",
      "MER-C\n",
      "****\n",
      "331691\n",
      "Jjron\n",
      "****\n",
      "------------------------------------------\n",
      "331777\n",
      "MER-C\n",
      "****\n",
      "------------------------------------------\n",
      "331800\n",
      "MER-C\n",
      "****\n",
      "331801\n",
      "Jjron\n",
      "****\n",
      "------------------------------------------\n",
      "331649\n",
      "MER-C\n",
      "****\n",
      "------------------------------------------\n",
      "331818\n",
      "MER-C\n",
      "****\n",
      "331819\n",
      "Jjron\n",
      "****\n",
      "------------------------------------------\n",
      "331785\n",
      "MER-C\n",
      "****\n",
      "331786\n",
      "Jjron\n",
      "****\n",
      "------------------------------------------\n",
      "331783\n",
      "MER-C\n",
      "****\n",
      "331784\n",
      "Jjron\n",
      "****\n",
      "------------------------------------------\n",
      "('Jack Merridew', 'Seraphimblade')\n",
      "496751\n",
      "Jack Merridew\n",
      "****\n",
      "496752\n",
      "Seraphimblade\n",
      "****\n",
      "------------------------------------------\n",
      "('Jimmuldrow', 'Adavidb')\n",
      "152955\n",
      "Jimmuldrow\n",
      "****\n",
      "152956\n",
      "Adavidb\n",
      "****\n",
      "------------------------------------------\n",
      "('Juhachi', 'Quasirandom')\n",
      "193381\n",
      "Juhachi\n",
      "****\n",
      "193382\n",
      "Quasirandom\n",
      "****\n",
      "193383\n",
      "Juhachi\n",
      "****\n",
      "------------------------------------------\n",
      "193408\n",
      "Juhachi\n",
      "****\n",
      "193409\n",
      "Quasirandom\n",
      "****\n",
      "------------------------------------------\n",
      "('Cyberia23', 'WikiuserNI')\n",
      "212671\n",
      "Cyberia23\n",
      "****\n",
      "212672\n",
      "WikiuserNI\n",
      "****\n",
      "212673\n",
      "Cyberia23\n",
      "****\n",
      "212674\n",
      "WikiuserNI\n",
      "****\n",
      "------------------------------------------\n",
      "('Bread Ninja', 'Axem Titanium')\n",
      "72277\n",
      "Bread Ninja\n",
      "****\n",
      "72278\n",
      "Axem Titanium\n",
      "****\n",
      "72279\n",
      "Bread Ninja\n",
      "****\n",
      "------------------------------------------\n",
      "72283\n",
      "Bread Ninja\n",
      "****\n",
      "72284\n",
      "Axem Titanium\n",
      "****\n",
      "72285\n",
      "Bread Ninja\n",
      "****\n",
      "72286\n",
      "Axem Titanium\n",
      "****\n",
      "------------------------------------------\n",
      "72280\n",
      "Bread Ninja\n",
      "****\n",
      "72281\n",
      "Axem Titanium\n",
      "****\n",
      "72282\n",
      "Bread Ninja\n",
      "****\n",
      "------------------------------------------\n",
      "72272\n",
      "Bread Ninja\n",
      "****\n",
      "72273\n",
      "Axem Titanium\n",
      "****\n",
      "72274\n",
      "Bread Ninja\n",
      "****\n",
      "72275\n",
      "Axem Titanium\n",
      "****\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print '# conversations = ',len(final_conversations.keys())\n",
    "\n",
    "users = user_pairs.keys()[20:30]\n",
    "\n",
    "print users[0]\n",
    "\n",
    "print '------------------------------'\n",
    "\n",
    "\n",
    "for user_pair in users:\n",
    "    print user_pair\n",
    "    for conv in final_conversations[user_pair]:\n",
    "        for u_id in conv:\n",
    "            print u_id\n",
    "            print utterances[u_id]['user_id']\n",
    "            # print utterances[utterance]['text']\n",
    "            print '****'\n",
    "\n",
    "        print '------------------------------------------'    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def append_to_left_of(item, li, item_to_insert, conv_list):\n",
    "    if(item in li):\n",
    "        index= li.index(item)\n",
    "        li.insert(index, item_to_insert)\n",
    "        conv_list.remove\n",
    "\n",
    "def make_conv_list(conv_list):\n",
    "    cur_conv = [conv_list[0]]\n",
    "    for c in range(1,len(conv_list)):\n",
    "        reply_to = utterances[c][\"reply_to\"]\n",
    "        \n",
    "\n",
    "conv_list = ['445339','445338','445337','445336','445335','445344','445345','445342','445343','445340','445341']\n",
    "\n",
    "def get_ordered_conversation(conv_list):\n",
    "    temp = []\n",
    "    while(len(conv_list) > 0):\n",
    "        conv_list = []\n",
    "        make_conv_list(conv_list)\n",
    "    \n",
    "    for c in conv_list:\n",
    "        reply_to = utterances[c][\"reply_to\"]\n",
    "        append_to_left_of\n",
    "        print reply_to\n",
    "        \n",
    "        \n",
    "get_ordered_conversation(conv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_left = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conversation_threads = {}\n",
    "for i, v in conversations.iteritems():\n",
    "    root_user = utterances[i][\"user_id\"]\n",
    "    other_user = get_other_user(v, root_user)\n",
    "    conversation_threads[(root_user, other_user)] = get_ordered_conversation(v)\n",
    "conversation_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk import sent_tokenize,word_tokenize,porter\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import ptb\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "import csv\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'although', 'and', 'as', 'as far as', 'as how', 'as if', 'as long as', 'as soon as', 'as though', 'as well as', 'because', 'before', 'both', 'but', 'either', 'even if', 'even though', 'for', 'how', 'however', 'if', 'if only', 'in case', 'in order that', 'neither', 'nor', 'now', 'once', 'only', 'or', 'provided', 'rather than', 'since', 'so', 'so that', 'than', 'that', 'though', 'till', 'unless', 'until', 'when', 'whenever', 'where', 'whereas', 'wherever', 'whether', 'while', 'yet']\n"
     ]
    }
   ],
   "source": [
    "def word_proc(x, preProcessing_type):\n",
    "    \n",
    "    if (preProcessing_type == 0): return x\n",
    "    if (preProcessing_type == 1): return x.lower()\n",
    "    if (preProcessing_type == 2): \n",
    "        stemmer = PorterStemmer()\n",
    "        x = x.lower()\n",
    "        return stemmer.stem(x)\n",
    "\n",
    "    return x # default \n",
    "\n",
    "def create_markers(category):\n",
    "\n",
    "    file = open(category+'.txt', \"r\")\n",
    "    markers = file.read()\n",
    "    \n",
    "    tokens = (re.split('[\\t\\n]', markers.strip()))\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "\n",
    "conjunction_list = create_markers('conjunctions')  \n",
    "\n",
    "print conjunction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown corpus\n"
     ]
    }
   ],
   "source": [
    "marker_list = {}\n",
    "utterances_tokenized = {}\n",
    "\n",
    "\"\"\" \n",
    "for consistency with prior work, we employed eight of the nine\n",
    "LIWC-derived categories [41] deemed to be processed by humans\n",
    "in a generally non-conscious fashion [28]. Our eight markers are\n",
    "thus: articles, auxiliary verbs, conjunctions, high-frequency ad-\n",
    "verbs, impersonal pronouns, personal pronouns, prepositions, and\n",
    "quantifiers (451 lexemes total)\n",
    "\"\"\"    \n",
    "\n",
    "\"\"\"\n",
    "I suggest using POS-labeled data to compute the most frequent words in each category, \n",
    "and then just make a cutoff. Most of these categories are closed-class, so your cutoff can be zero,\n",
    "except for the adverbs.\n",
    "\n",
    "Tagged data is available with NLTK.\n",
    "\n",
    "I think the Brown corpus tagset distinguishes quantifiers and the PTB doesn't, \n",
    "but I'm not sure; you can check. NLTK should give access to a small subset of these corpora, \n",
    "but it should be enough for you to build a dictionary. articles are sometimes called \"Determiners\" \n",
    "in these sorts of corpora.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tag_lists = {}\n",
    "tag_counter = Counter()\n",
    "adverb_word_counter = Counter()\n",
    "\n",
    "def populateMarkers():\n",
    "    \n",
    "    print 'Brown corpus'\n",
    "    # print brown.words()[0:10]\n",
    "    # print brown.tagged_paras()[0:2]\n",
    "    \n",
    "    tagged_words = treebank.tagged_words()\n",
    "    \n",
    "\n",
    "    \n",
    "    # print '#words',len(tagged_words)\n",
    "    \n",
    "    for pair in tagged_words:\n",
    "        tag  = pair[1]\n",
    "        word = word_proc(pair[0], 1)\n",
    "        \n",
    "        if tag not in tag_lists:\n",
    "            tag_lists[tag] = []\n",
    "        \n",
    "        if (word not in tag_lists[tag]):\n",
    "            tag_lists[tag].append(word)\n",
    "    \n",
    "        if ((tag == 'RB') or (tag == 'RBR') or (tag == 'RBS') or (tag == 'WRB')):\n",
    "            adverb_word_counter[word] +=1\n",
    "    \n",
    "    \n",
    "    tagged_words = brown.tagged_words()\n",
    "    tag_lists['QUANT'] = []\n",
    "    # print '#words',len(tagged_words)\n",
    "    \n",
    "    for pair in tagged_words:\n",
    "        tag  = pair[1]\n",
    "        word = word_proc(pair[0], 1)\n",
    "        \n",
    "        if ((tag == 'ABN') or (tag == 'ABX')):\n",
    "            if (word not in tag_lists['QUANT']):\n",
    "                tag_lists['QUANT'].append(word)\n",
    "    \n",
    "    \n",
    "populateMarkers()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marker_list['articles'] =  tag_lists['DT']\n",
    "marker_list['personal_pronouns'] = tag_lists['PRP']\n",
    "marker_list['impersonal_pronouns'] = tag_lists['PRP$'] + tag_lists['WP'] + tag_lists['WP$']\n",
    "marker_list['conjunctions'] = tag_lists['CC'] + tag_lists['IN']\n",
    "marker_list['prepositions'] = tag_lists['IN']\n",
    "marker_list['auxillary_verbs'] = tag_lists['MD']\n",
    "marker_list['quantifiers'] = tag_lists['QUANT']\n",
    "\n",
    "#  for now considering 100 most commonly occurring adverbs\n",
    "high_freq_adverb_counts = adverb_word_counter.most_common(100)\n",
    "\n",
    "marker_list['adverbs'] = []\n",
    "\n",
    "for adverb_count_pair in high_freq_adverb_counts:\n",
    "    marker_list['adverbs'].append(adverb_count_pair[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': 0.0, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': -10, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': -10, 'prepositions': -10, 'personal_pronouns': -10, 'adverbs': -10, 'auxillary_verbs': -10}\n",
      "------------------------------------------\n",
      "{'articles': -10, 'impersonal_pronouns': 0.0, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': -10, 'adverbs': 0.0, 'auxillary_verbs': -10}\n",
      "------------------------------------------\n",
      "{'articles': -10, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': -10, 'prepositions': -10, 'personal_pronouns': -10, 'adverbs': -10, 'auxillary_verbs': -10}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': 0.0, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': -0.5}\n",
      "------------------------------------------\n",
      "{'articles': -10, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': -10, 'prepositions': -10, 'personal_pronouns': -10, 'adverbs': -10, 'auxillary_verbs': -10}\n",
      "------------------------------------------\n",
      "{'articles': -10, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': -10, 'prepositions': -10, 'personal_pronouns': -10, 'adverbs': -10, 'auxillary_verbs': -10}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': 0.0, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': 0.0, 'quantifiers': 0.0, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': -10}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': -10}\n",
      "------------------------------------------\n",
      "{'articles': -0.33333333333333337, 'impersonal_pronouns': -0.25, 'quantifiers': 0.0, 'conjunctions': -0.33333333333333337, 'prepositions': -0.33333333333333337, 'personal_pronouns': -0.4, 'adverbs': -0.33333333333333337, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': 0.0, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': -0.4, 'impersonal_pronouns': 0.0, 'quantifiers': -10, 'conjunctions': -0.4, 'prepositions': -0.4, 'personal_pronouns': -0.25, 'adverbs': -0.25, 'auxillary_verbs': -0.16666666666666663}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': 0.33333333333333337, 'quantifiers': -10, 'conjunctions': -0.25, 'prepositions': -0.25, 'personal_pronouns': -0.33333333333333337, 'adverbs': -0.16666666666666663, 'auxillary_verbs': -0.25}\n",
      "------------------------------------------\n",
      "{'articles': 0.0, 'impersonal_pronouns': 0.0, 'quantifiers': -10, 'conjunctions': 0.0, 'prepositions': 0.0, 'personal_pronouns': 0.0, 'adverbs': 0.0, 'auxillary_verbs': 0.0}\n",
      "------------------------------------------\n",
      "{'articles': -10, 'impersonal_pronouns': -10, 'quantifiers': -10, 'conjunctions': -10, 'prepositions': -10, 'personal_pronouns': -10, 'adverbs': -10, 'auxillary_verbs': -10}\n"
     ]
    }
   ],
   "source": [
    "# code to compute coordination between user a and user b\n",
    "\n",
    "\"\"\"('Tpbradbury', 'Karanacs'): [['357026', '357027', '357028', '357029']], \n",
    "    ('Skope', 'Jedi6'):         [['500004', '500005'], ['375918', '375919'], ['500181', '500183'], \n",
    "                                 ['500029', '500030', '500031', '500032']]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "utterances = {}\n",
    "                utterances[split[0].strip()] = {\"user_id\": split[1].strip(),\n",
    "                                        \"talkpage_user\": split[2].strip(), \n",
    "                                        \"conversation_root\": split[3].strip(),\n",
    "                                        \"reply_to\": split[4].strip(),\n",
    "                                        \"timestamp\": split[5].strip(),\n",
    "                                        \"timestamp_unix\": split[6].strip(),\n",
    "#                                       looks like readme has a small mistake                                     \n",
    "                                        \"raw_text\": split[7].strip(),\n",
    "                                        \"text\": split[8].strip(),        \n",
    "                                        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(utterance):\n",
    "    preProcessing_type = 1\n",
    "    \n",
    "    text = utterances[utterance][\"text\"]\n",
    "    \n",
    "    word_list = (text.strip()).split(' ')\n",
    "    \n",
    "    tokens = Counter()\n",
    "    \n",
    "    \n",
    "    # print 'text',text\n",
    "    \n",
    "    # for word in word_tokenize(text):\n",
    "    for word in word_list:\n",
    "        tokens[word_proc(word, preProcessing_type)] += 1\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def contains(utterance, marker):\n",
    "    \n",
    "    preProcessing_type = 1\n",
    "    \n",
    "    if (utterance not in utterances_tokenized):\n",
    "        utterances_tokenized[utterance] = tokenize(utterance) # returns dictionary with keys as tokens\n",
    "        \n",
    "    # print utterances_tokenized\n",
    "\n",
    "    # print \"--------------------------------------\"\n",
    "    \n",
    "    word_list = marker_list[marker]\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        word = word_proc(word_list[i], preProcessing_type)\n",
    "        if (word in utterances_tokenized[utterance]):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input: list of utterance list. Each utterance list is a conversation   \n",
    "def compute_coordination(conv_list):\n",
    "    \n",
    "    # marker_list['articles'] =  ['a', 'an', 'the']\n",
    "    # marker_list['personal_pronouns'] = ['i','them','her','you','he','she','it','they','we']\n",
    "    # marker_list['conjunctions'] = create_markers('conjunctions')\n",
    "    \n",
    "    \n",
    "    coordination_list = {}\n",
    "    \n",
    "    for marker in marker_list.keys():\n",
    "        \n",
    "        a_usage_count = 0\n",
    "        b_usage_count = 0\n",
    "        \n",
    "        b_follows_a_count = 0\n",
    "        b_utterance_count = 0\n",
    "        \n",
    "        for conversation in conv_list:\n",
    "            for i in range(len(conversation)): #['500029', '500030', '500031', '500032']\n",
    "                \n",
    "                # print 'i',i\n",
    "                \n",
    "                if (i%2 != 0):continue # for skipping B's utterance\n",
    "                    \n",
    "                 \n",
    "                a_utterance = conversation[i]\n",
    "                b_utterance = None\n",
    "                if (i+1 < len(conversation)):\n",
    "                        b_utterance = conversation[i+1]\n",
    "                        b_utterance_count +=1\n",
    "            \n",
    "                a_utterance_has_m = contains(a_utterance, marker)\n",
    "                \n",
    "                b_utterance_has_m = False\n",
    "                if (b_utterance != None):\n",
    "                    b_utterance_has_m = contains(b_utterance, marker)\n",
    "                \n",
    "                if (a_utterance_has_m):\n",
    "                    a_usage_count +=1\n",
    "                    \n",
    "                if (b_utterance_has_m):    \n",
    "                    b_usage_count +=1\n",
    "                    \n",
    "                if (a_utterance_has_m and b_utterance_has_m):\n",
    "                    b_follows_a_count +=1\n",
    "       \n",
    "    \n",
    "#        print 'marker',marker\n",
    "#        print 'b_follows_a_count',b_follows_a_count\n",
    "#        print 'a_usage_count',a_usage_count \n",
    "#        print  'b_usage_count',b_usage_count\n",
    "#        print 'b_utterance_count',b_utterance_count \n",
    "#        print '----------------------'\n",
    "        if (a_usage_count == 0 or b_utterance_count == 0): \n",
    "            coordination = -10 # invalid\n",
    "        else:\n",
    "            coordination = 1.0*b_follows_a_count/a_usage_count - 1.0*b_usage_count/b_utterance_count\n",
    "        \n",
    "#       if ((a_usage_count != 0) and (b_utterance_count !=0) ): \n",
    "#            print 'follows',1.0*b_follows_a_count/a_usage_count\n",
    "#            print 'base rate',1.0*b_usage_count/b_utterance_count\n",
    "        \n",
    "        coordination_list[marker] = coordination\n",
    "                # if ()\n",
    "    \n",
    "    return coordination_list\n",
    "   \n",
    "\n",
    "# test code : tested manually    \n",
    "# print compute_coordination(final_conversations[('Skope', 'Jedi6')])\n",
    "\n",
    "# for conv in final_conversations[('Skope', 'Jedi6')]:\n",
    "#    for utterance in conv:\n",
    "#        print utterances[utterance]['user_id']\n",
    "#        print utterances[utterance]['text']\n",
    "#        print '****'\n",
    "        \n",
    "#    print '----------------------'    \n",
    "    \n",
    "#        \n",
    "# test code : -10 indicates cannot be computed \n",
    "i = 0\n",
    "for key in final_conversations.keys():    \n",
    "    i +=1\n",
    "    if (i > 20): break\n",
    "    # print 'conv_list',final_conversations[key]\n",
    "    \n",
    "    print '------------------------------------------'\n",
    "    print compute_coordination(final_conversations[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parul's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "admin_data = {}\n",
    "admin_file = open(\"wikipedia.talkpages.admins.txt\", 'r')\n",
    "for line in admin_file:\n",
    "    cols = line.split(\" \")\n",
    "    if len(cols) == 2:\n",
    "        admin_data[cols[0]] = cols[1].split(\"\\n\")[0]\n",
    "    else:\n",
    "        name = \"\"\n",
    "        for i in range(len(cols)-2):\n",
    "            name += cols[i]+\" \"\n",
    "        name+= cols[i+1]\n",
    "        admin_data[name] = cols[len(cols)-1].split(\"\\n\")[0]\n",
    "#print admin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26397\n"
     ]
    }
   ],
   "source": [
    "user_data = {}\n",
    "user_file = open(\"wikipedia.talkpages.userinfo.txt\", 'r')\n",
    "for line in user_file:\n",
    "    cols = line.split(\" +++$+++ \")\n",
    "    user_data[cols[0]] = cols[2]#.split(\"\\n\")[0]\n",
    "print len(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate1(cbA):\n",
    "    total = 0\n",
    "    i = 0\n",
    "    #print cbA\n",
    "    for coord in cbA.values():\n",
    "        c_b_a = coord.values()\n",
    "        if -10 not in c_b_a:\n",
    "            i+=1\n",
    "            total += np.average(c_b_a)\n",
    "    CBA = total/i\n",
    "    return CBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate2(cbA):\n",
    "    total = np.array([0.0,0.0,0.0]) #,0.0,0.0,0.0,0.0,0.0])\n",
    "    counts = np.zeros((1,3))\n",
    "    i = 0\n",
    "    for coord in cbA.values():\n",
    "        c_b_a = coord.values()\n",
    "        np_cba = np.array(c_b_a)\n",
    "        total[np_cba != -10] += np_cba[np_cba != -10]\n",
    "        #print (np_cba != 0).astype(int)\n",
    "        #print np_cba[np_cba != 0]\n",
    "        counts += (np_cba != -10).astype(int)\n",
    "    #print counts[counts != 0]\n",
    "    #print total[total != 0]\n",
    "    if 0 not in counts:\n",
    "        CBA = np.average(total/counts)\n",
    "    else:\n",
    "        CBA = np.sum(total[total != -10]/counts[counts != -10])/3#8\n",
    "    return CBA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate3(cbA):\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for coord in cbA.values():\n",
    "        c_b_a = coord.values()\n",
    "        np_cba = np.array(c_b_a)\n",
    "        #print np_cba[np_cba != -10]\n",
    "        if np_cba[np_cba != -10] != []:\n",
    "            total += np.average(np_cba[np_cba != -10])\n",
    "        \n",
    "    #print len(cbA)\n",
    "    #print total\n",
    "    CBA = total/len(cbA)\n",
    "    return CBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82801\n",
      "0.249127425997\n",
      "0.750872574003\n"
     ]
    }
   ],
   "source": [
    "a_admin = 0\n",
    "a_notadmin = 0\n",
    "\n",
    "print len(final_conversations)\n",
    "\n",
    "for (a,b), conversation in final_conversations.items():\n",
    "    if a in admin_data:\n",
    "        a_admin+=1\n",
    "    else: \n",
    "        a_notadmin+=1\n",
    "        \n",
    "print a_admin/float(len(final_conversations))\n",
    "print a_notadmin/float(len(final_conversations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getConversations(user):\n",
    "    all_converation = {'admin':[], 'non_admin': [], 'U': []}\n",
    "    for (a,b), conversation in final_conversations.items():\n",
    "        if b == user:\n",
    "            if a in admin_data:\n",
    "                all_converation['admin'] += conversation\n",
    "            elif a not in admin_data:\n",
    "                all_converation['non_admin'] += conversation\n",
    "            all_converation['U'] += conversation\n",
    "    return all_converation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construct C(U,G_high)\n",
    "C_U_G_high = {}\n",
    "C_U_G_low = {}\n",
    "C_G_high_U = {}\n",
    "C_G_low_U = {}\n",
    "\n",
    "i = 0\n",
    "for (a,b), conversation in final_conversations.items():\n",
    "    if i > 1000:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    if (\"admin\",b) not in C_U_G_high and (\"non_admin\",b) not in C_U_G_low: \n",
    "        all_conversation = getConversations(b)\n",
    "        i+=1\n",
    "        \n",
    "    if a in admin_data:\n",
    "        if (\"admin\",b) not in C_U_G_high:\n",
    "            C_U_G_high[(\"admin\",b)]= compute_coordination(all_conversation['admin'])\n",
    "        '''  \n",
    "        new_conversation = []\n",
    "        for thread in conversation:\n",
    "            thread = thread[1:len(thread)]\n",
    "            new_conversation.append(thread)\n",
    "        C_G_high_U[(b,a)]= compute_coordination(new_conversation)\n",
    "        '''\n",
    "\n",
    "    elif a not in admin_data:\n",
    "        if (\"non_admin\",b) not in C_U_G_low:\n",
    "            C_U_G_low[(\"non_admin\",b)]= compute_coordination(all_conversation['non_admin'])\n",
    "        '''\n",
    "        new_conversation = []\n",
    "        for thread in conversation:\n",
    "            thread = thread[1:len(thread)]\n",
    "            new_conversation.append(thread)\n",
    "        C_G_low_U[(b,a)]= compute_coordination(new_conversation)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "    if b in admin_data:\n",
    "        if (\"U\",b) not in C_G_high_U:\n",
    "            C_G_high_U[(\"U\",b)]= compute_coordination(all_conversation['U'])\n",
    "        '''\n",
    "        new_conversation = []\n",
    "        for thread in conversation:\n",
    "            thread = thread[1:len(thread)]\n",
    "            new_conversation.append(thread)\n",
    "        C_U_G_high[(b,a)]= compute_coordination(new_conversation)\n",
    "        '''\n",
    "    \n",
    "    elif b not in admin_data:\n",
    "        if (\"U\",b) not in C_G_low_U:\n",
    "            C_G_low_U[(\"U\",b)]= compute_coordination(all_conversation['U'])\n",
    "        '''\n",
    "        new_conversation = []\n",
    "        for thread in conversation:\n",
    "            thread = thread[1:len(thread)]\n",
    "            new_conversation.append(thread)\n",
    "        C_U_G_low[(b,a)]= compute_coordination(new_conversation)\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "{'C_U_G_high': -0.1160971671903919, 'C_U_G_low': -0.10927522027035985, 'C_G_high_U': -0.11591807480568213, 'C_G_low_U': -0.1410598476165344}\n",
      "{'C_U_G_high': -0.11609716719039191, 'C_U_G_low': -0.10927522027035985, 'C_G_high_U': -0.11591807480568207, 'C_G_low_U': -0.14105984761653437}\n",
      "{'C_U_G_high': -0.10964732456870346, 'C_U_G_low': -0.10927522027035985, 'C_G_high_U': -0.11591807480568213, 'C_G_low_U': -0.1410598476165344}\n"
     ]
    }
   ],
   "source": [
    "groups = ['C_U_G_high','C_U_G_low','C_G_high_U','C_G_low_U']\n",
    "group_results = [C_U_G_high, C_U_G_low , C_G_high_U , C_G_low_U]\n",
    "\n",
    "print len(C_U_G_high)\n",
    "\n",
    "agg1 = {}\n",
    "agg2 = {}\n",
    "agg3 = {}\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    agg1[groups[i]] = aggregate1(group_results[i])\n",
    "    agg2[groups[i]] = aggregate2(group_results[i])\n",
    "    agg3[groups[i]] = aggregate3(group_results[i])\n",
    "    \n",
    "print agg1\n",
    "print agg2\n",
    "print agg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
